{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IBM_Transformer+TimeEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wileyw/DeepLearningDemos/blob/master/handwriting_generator/IBM_Transformer%2BTimeEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfXx2MJcwLSk",
        "colab_type": "text"
      },
      "source": [
        "# Notebook\n",
        "Original code from here: [code](https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb)\n",
        "Data from here: [data](https://finance.yahoo.com/quote/IBM/history?p=IBM)\n",
        "Original post from here: [post](https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6)\n",
        "\n",
        "We're just running the original code in this notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6zxz6xZyWb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFSM9LT54LR",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acgb31zy5lcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "seq_len = 128\n",
        "\n",
        "d_k = 256\n",
        "d_v = 256\n",
        "n_heads = 12\n",
        "ff_dim = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnPzNUPK50Ki",
        "colab_type": "text"
      },
      "source": [
        "## Load IBM data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOaXlWbQlNjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/wileyw/DeepLearningDemos.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DVMJjRllRy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls DeepLearningDemos/handwriting_generator/IBM.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdi13JvV5ltd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IBM_path = 'DeepLearningDemos/handwriting_generator/IBM.csv'\n",
        "\n",
        "df = pd.read_csv(IBM_path, delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "# Replace 0 to avoid dividing by 0 later on\n",
        "df['Volume'].replace(to_replace=0, method='ffill', inplace=True) \n",
        "df.sort_values('Date', inplace=True)\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Caac4a66588l",
        "colab_type": "text"
      },
      "source": [
        "## Plot daily IBM closing prices and volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ1tN1_k5lx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,10))\n",
        "st = fig.suptitle(\"IBM Close Price and Volume\", fontsize=20)\n",
        "st.set_y(0.92)\n",
        "\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax1.plot(df['Close'], label='IBM Close Price')\n",
        "ax1.set_xticks(range(0, df.shape[0], 1464))\n",
        "ax1.set_xticklabels(df['Date'].loc[::1464])\n",
        "ax1.set_ylabel('Close Price', fontsize=18)\n",
        "ax1.legend(loc=\"upper left\", fontsize=12)\n",
        "\n",
        "ax2 = fig.add_subplot(212)\n",
        "ax2.plot(df['Volume'], label='IBM Volume')\n",
        "ax2.set_xticks(range(0, df.shape[0], 1464))\n",
        "ax2.set_xticklabels(df['Date'].loc[::1464])\n",
        "ax2.set_ylabel('Volume', fontsize=18)\n",
        "ax2.legend(loc=\"upper left\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTS3Cm2H6GBz",
        "colab_type": "text"
      },
      "source": [
        "## Calculate normalized percentage change of all columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7_Uju9B5lqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Calculate percentage change'''\n",
        "\n",
        "df['Open'] = df['Open'].pct_change() # Create arithmetic returns column\n",
        "df['High'] = df['High'].pct_change() # Create arithmetic returns column\n",
        "df['Low'] = df['Low'].pct_change() # Create arithmetic returns column\n",
        "df['Close'] = df['Close'].pct_change() # Create arithmetic returns column\n",
        "df['Volume'] = df['Volume'].pct_change()\n",
        "\n",
        "df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "\n",
        "###############################################################################\n",
        "'''Normalize price columns'''\n",
        "\n",
        "min_return = min(df[['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "max_return = max(df[['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "# Min-max normalize price columns (0-1 range)\n",
        "df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "###############################################################################\n",
        "'''Normalize volume column'''\n",
        "\n",
        "min_volume = df['Volume'].min(axis=0)\n",
        "max_volume = df['Volume'].max(axis=0)\n",
        "\n",
        "# Min-max normalize volume columns (0-1 range)\n",
        "df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "###############################################################################\n",
        "'''Create training, validation and test split'''\n",
        "\n",
        "times = sorted(df.index.values)\n",
        "last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n",
        "last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n",
        "\n",
        "df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
        "df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
        "df_test = df[(df.index >= last_10pct)]\n",
        "\n",
        "# Remove date column\n",
        "df_train.drop(columns=['Date'], inplace=True)\n",
        "df_val.drop(columns=['Date'], inplace=True)\n",
        "df_test.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "# Convert pandas columns into arrays\n",
        "train_data = df_train.values\n",
        "val_data = df_val.values\n",
        "test_data = df_test.values\n",
        "print('Training data shape: {}'.format(train_data.shape))\n",
        "print('Validation data shape: {}'.format(val_data.shape))\n",
        "print('Test data shape: {}'.format(test_data.shape))\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diGgUw_f6fAI",
        "colab_type": "text"
      },
      "source": [
        "## Plot daily changes of close prices and volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJCti1mt5lnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,12))\n",
        "st = fig.suptitle(\"Data Separation\", fontsize=20)\n",
        "st.set_y(0.95)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax1.plot(np.arange(train_data.shape[0]), df_train['Close'], label='Training data')\n",
        "\n",
        "ax1.plot(np.arange(train_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]), df_val['Close'], label='Validation data')\n",
        "\n",
        "ax1.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Close'], label='Test data')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Normalized Closing Returns')\n",
        "ax1.set_title(\"Close Price\", fontsize=18)\n",
        "ax1.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "ax2 = fig.add_subplot(212)\n",
        "ax2.plot(np.arange(train_data.shape[0]), df_train['Volume'], label='Training data')\n",
        "\n",
        "ax2.plot(np.arange(train_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]), df_val['Volume'], label='Validation data')\n",
        "\n",
        "ax2.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Volume'], label='Test data')\n",
        "ax2.set_xlabel('Date')\n",
        "ax2.set_ylabel('Normalized Volume Changes')\n",
        "ax2.set_title(\"Volume\", fontsize=18)\n",
        "ax2.legend(loc=\"best\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-9JDha_6l3L",
        "colab_type": "text"
      },
      "source": [
        "## Create chunks of training, validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib8wOc-_5llL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training data\n",
        "X_train, y_train = [], []\n",
        "for i in range(seq_len, len(train_data)):\n",
        "  X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n",
        "  y_train.append(train_data[:, 3][i]) #Value of 4th column (Close Price) of df-row 128+1\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Validation data\n",
        "X_val, y_val = [], []\n",
        "for i in range(seq_len, len(val_data)):\n",
        "    X_val.append(val_data[i-seq_len:i])\n",
        "    y_val.append(val_data[:, 3][i])\n",
        "X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Test data\n",
        "X_test, y_test = [], []\n",
        "for i in range(seq_len, len(test_data)):\n",
        "    X_test.append(test_data[i-seq_len:i])\n",
        "    y_test.append(test_data[:, 3][i])    \n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "print('Training set shape', X_train.shape, y_train.shape)\n",
        "print('Validation set shape', X_val.shape, y_val.shape)\n",
        "print('Testing set shape' ,X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WhTPe6I6sDu",
        "colab_type": "text"
      },
      "source": [
        "## TimeVector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUawOlTD5ljR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Time2Vector(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    super(Time2Vector, self).__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    '''Initialize weights and biases with shape (batch, seq_len)'''\n",
        "    self.weights_linear = self.add_weight(name='weight_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "    \n",
        "    self.bias_linear = self.add_weight(name='bias_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "    \n",
        "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    '''Calculate linear and periodic time features'''\n",
        "    x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
        "    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
        "    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
        "    \n",
        "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
        "    time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
        "    return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
        "   \n",
        "  def get_config(self): # Needed for saving and loading model with custom layer\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'seq_len': self.seq_len})\n",
        "    return config\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJZOW8d56wyJ",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Veb1wu_5lhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SingleAttention(Layer):\n",
        "  def __init__(self, d_k, d_v):\n",
        "    super(SingleAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.query = Dense(self.d_k, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "    \n",
        "    self.key = Dense(self.d_k, \n",
        "                     input_shape=input_shape, \n",
        "                     kernel_initializer='glorot_uniform', \n",
        "                     bias_initializer='glorot_uniform')\n",
        "    \n",
        "    self.value = Dense(self.d_v, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "    q = self.query(inputs[0])\n",
        "    k = self.key(inputs[1])\n",
        "\n",
        "    attn_weights = tf.matmul(q, k, transpose_b=True)\n",
        "    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
        "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
        "    \n",
        "    v = self.value(inputs[2])\n",
        "    attn_out = tf.matmul(attn_weights, v)\n",
        "    return attn_out    \n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class MultiAttention(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads):\n",
        "    super(MultiAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.attn_heads = list()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    for n in range(self.n_heads):\n",
        "      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
        "    \n",
        "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
        "    self.linear = Dense(input_shape[0][-1], \n",
        "                        input_shape=input_shape, \n",
        "                        kernel_initializer='glorot_uniform', \n",
        "                        bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
        "    concat_attn = tf.concat(attn, axis=-1)\n",
        "    multi_linear = self.linear(concat_attn)\n",
        "    return multi_linear   \n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    self.attn_heads = list()\n",
        "    self.dropout_rate = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
        "    self.attn_dropout = Dropout(self.dropout_rate)\n",
        "    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
        "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
        "    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
        "    self.ff_dropout = Dropout(self.dropout_rate)\n",
        "    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
        "  \n",
        "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "    attn_layer = self.attn_multi(inputs)\n",
        "    attn_layer = self.attn_dropout(attn_layer)\n",
        "    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
        "\n",
        "    ff_layer = self.ff_conv1D_1(attn_layer)\n",
        "    ff_layer = self.ff_conv1D_2(ff_layer)\n",
        "    ff_layer = self.ff_dropout(ff_layer)\n",
        "    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
        "    return ff_layer \n",
        "\n",
        "  def get_config(self): # Needed for saving and loading model with custom layer\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'d_k': self.d_k,\n",
        "                   'd_v': self.d_v,\n",
        "                   'n_heads': self.n_heads,\n",
        "                   'ff_dim': self.ff_dim,\n",
        "                   'attn_heads': self.attn_heads,\n",
        "                   'dropout_rate': self.dropout_rate})\n",
        "    return config          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtBanXmQB4jF",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1z9GgwV5lZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f3d93f91-0f2b-45ab-cddb-89a54f86f623"
      },
      "source": [
        "def create_model():\n",
        "  '''Initialize time and transformer layers'''\n",
        "  time_embedding = Time2Vector(seq_len)\n",
        "  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "\n",
        "  '''Construct model'''\n",
        "  in_seq = Input(shape=(seq_len, 5))\n",
        "  x = time_embedding(in_seq)\n",
        "  x = Concatenate(axis=-1)([in_seq, x])\n",
        "  x = attn_layer1((x, x, x))\n",
        "  x = attn_layer2((x, x, x))\n",
        "  x = attn_layer3((x, x, x))\n",
        "  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  out = Dense(1, activation='linear')(x)\n",
        "\n",
        "  model = Model(inputs=in_seq, outputs=out)\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
        "  return model\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "callback = tf.keras.callbacks.ModelCheckpoint('Transformer+TimeEmbedding.hdf5', \n",
        "                                              monitor='val_loss', \n",
        "                                              save_best_only=True, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, \n",
        "                    batch_size=batch_size, \n",
        "                    epochs=35, \n",
        "                    callbacks=[callback],\n",
        "                    validation_data=(X_val, y_val))  \n",
        "\n",
        "model = tf.keras.models.load_model('/content/Transformer+TimeEmbedding.hdf5',\n",
        "                                   custom_objects={'Time2Vector': Time2Vector, \n",
        "                                                   'SingleAttention': SingleAttention,\n",
        "                                                   'MultiAttention': MultiAttention,\n",
        "                                                   'TransformerEncoder': TransformerEncoder})\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "'''Calculate predictions and metrics'''\n",
        "\n",
        "#Calculate predication for training, validation and test data\n",
        "train_pred = model.predict(X_train)\n",
        "val_pred = model.predict(X_val)\n",
        "test_pred = model.predict(X_test)\n",
        "\n",
        "#Print evaluation metrics for all datasets\n",
        "train_eval = model.evaluate(X_train, y_train, verbose=0)\n",
        "val_eval = model.evaluate(X_val, y_val, verbose=0)\n",
        "test_eval = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(' ')\n",
        "print('Evaluation metrics')\n",
        "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
        "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\n",
        "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))\n",
        "\n",
        "###############################################################################\n",
        "'''Display results'''\n",
        "\n",
        "fig = plt.figure(figsize=(15,20))\n",
        "st = fig.suptitle(\"Transformer + TimeEmbedding Model\", fontsize=22)\n",
        "st.set_y(0.92)\n",
        "\n",
        "#Plot training data results\n",
        "ax11 = fig.add_subplot(311)\n",
        "ax11.plot(train_data[:, 3], label='IBM Closing Returns')\n",
        "ax11.plot(np.arange(seq_len, train_pred.shape[0]+seq_len), train_pred, linewidth=3, label='Predicted IBM Closing Returns')\n",
        "ax11.set_title(\"Training Data\", fontsize=18)\n",
        "ax11.set_xlabel('Date')\n",
        "ax11.set_ylabel('IBM Closing Returns')\n",
        "ax11.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot validation data results\n",
        "ax21 = fig.add_subplot(312)\n",
        "ax21.plot(val_data[:, 3], label='IBM Closing Returns')\n",
        "ax21.plot(np.arange(seq_len, val_pred.shape[0]+seq_len), val_pred, linewidth=3, label='Predicted IBM Closing Returns')\n",
        "ax21.set_title(\"Validation Data\", fontsize=18)\n",
        "ax21.set_xlabel('Date')\n",
        "ax21.set_ylabel('IBM Closing Returns')\n",
        "ax21.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot test data results\n",
        "ax31 = fig.add_subplot(313)\n",
        "ax31.plot(test_data[:, 3], label='IBM Closing Returns')\n",
        "ax31.plot(np.arange(seq_len, test_pred.shape[0]+seq_len), test_pred, linewidth=3, label='Predicted IBM Closing Returns')\n",
        "ax31.set_title(\"Test Data\", fontsize=18)\n",
        "ax31.set_xlabel('Date')\n",
        "ax31.set_ylabel('IBM Closing Returns')\n",
        "ax31.legend(loc=\"best\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00080\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r365/365 [==============================] - 138s 377ms/step - loss: 0.0016 - mae: 0.0287 - mape: 5.1570 - val_loss: 8.3149e-04 - val_mae: 0.0209 - val_mape: 3.5907\n",
            "Epoch 21/35\n",
            "131/365 [=========>....................] - ETA: 1:21 - loss: 0.0016 - mae: 0.0287 - mape: 4.9202"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiCB_z-iCMMr",
        "colab_type": "text"
      },
      "source": [
        "## Model metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maSSTkJPCOE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Display model metrics'''\n",
        "\n",
        "fig = plt.figure(figsize=(15,20))\n",
        "st = fig.suptitle(\"Transformer + TimeEmbedding Model Metrics\", fontsize=22)\n",
        "st.set_y(0.92)\n",
        "\n",
        "#Plot model loss\n",
        "ax1 = fig.add_subplot(311)\n",
        "ax1.plot(history.history['loss'], label='Training loss (MSE)')\n",
        "ax1.plot(history.history['val_loss'], label='Validation loss (MSE)')\n",
        "ax1.set_title(\"Model loss\", fontsize=18)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss (MSE)')\n",
        "ax1.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot MAE\n",
        "ax2 = fig.add_subplot(312)\n",
        "ax2.plot(history.history['mae'], label='Training MAE')\n",
        "ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
        "ax2.set_title(\"Model metric - Mean average error (MAE)\", fontsize=18)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Mean average error (MAE)')\n",
        "ax2.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot MAPE\n",
        "ax3 = fig.add_subplot(313)\n",
        "ax3.plot(history.history['mape'], label='Training MAPE')\n",
        "ax3.plot(history.history['val_mape'], label='Validation MAPE')\n",
        "ax3.set_title(\"Model metric - Mean average percentage error (MAPE)\", fontsize=18)\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Mean average percentage error (MAPE)')\n",
        "ax3.legend(loc=\"best\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cvzV1ArCOwK",
        "colab_type": "text"
      },
      "source": [
        "## Model architecture overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b--iZiKCO5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    to_file=\"IBM_Transformer+TimeEmbedding.png\",\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    expand_nested=True,\n",
        "    dpi=96,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXm-L7Y7SXY4",
        "colab_type": "text"
      },
      "source": [
        "# Moving Average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWDD3gi0SkBT",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average - Load IBM data again, to apply rolling window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ShhyS6Sajz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IBM_path = 'DeepLearningDemos/handwriting_generator/IBM.csv'\n",
        "\n",
        "df = pd.read_csv(IBM_path, delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "# Replace 0 to avoid dividing by 0 later on\n",
        "df['Volume'].replace(to_replace=0, method='ffill', inplace=True) \n",
        "df.sort_values('Date', inplace=True)\n",
        "\n",
        "# Apply moving average with a window of 10 days to all columns\n",
        "df[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].rolling(10).mean() \n",
        "\n",
        "# Drop all rows with NaN values\n",
        "df.dropna(how='any', axis=0, inplace=True) \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh7gRX0FS-xZ",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average - Plot daily IBM closing prices and volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT97nI1sSafp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,10))\n",
        "st = fig.suptitle(\"IBM Close Price and Volume\", fontsize=20)\n",
        "st.set_y(0.92)\n",
        "\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax1.plot(df['Close'], label='IBM Close Price')\n",
        "ax1.set_xticks(range(0, df.shape[0], 1464))\n",
        "ax1.set_xticklabels(df['Date'].loc[::1464])\n",
        "ax1.set_ylabel('Close Price', fontsize=18)\n",
        "ax1.legend(loc=\"upper left\", fontsize=12)\n",
        "\n",
        "ax2 = fig.add_subplot(212)\n",
        "ax2.plot(df['Volume'], label='IBM Volume')\n",
        "ax2.set_xticks(range(0, df.shape[0], 1464))\n",
        "ax2.set_xticklabels(df['Date'].loc[::1464])\n",
        "ax2.set_ylabel('Volume', fontsize=18)\n",
        "ax2.legend(loc=\"upper left\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaGiU_wgTYqu",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average - Calculate normalized percentage change for all columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOQVdn5TSaZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Calculate percentage change'''\n",
        "\n",
        "df['Open'] = df['Open'].pct_change() # Create arithmetic returns column\n",
        "df['High'] = df['High'].pct_change() # Create arithmetic returns column\n",
        "df['Low'] = df['Low'].pct_change() # Create arithmetic returns column\n",
        "df['Close'] = df['Close'].pct_change() # Create arithmetic returns column\n",
        "df['Volume'] = df['Volume'].pct_change()\n",
        "\n",
        "df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "\n",
        "###############################################################################\n",
        "'''Normalize price columns'''\n",
        "\n",
        "min_return = min(df[['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "max_return = max(df[['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "# Min-max normalize price columns (0-1 range)\n",
        "df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "###############################################################################\n",
        "'''Normalize volume column'''\n",
        "\n",
        "min_volume = df['Volume'].min(axis=0)\n",
        "max_volume = df['Volume'].max(axis=0)\n",
        "\n",
        "# Min-max normalize volume columns (0-1 range)\n",
        "df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "###############################################################################\n",
        "'''Create training, validation and test split'''\n",
        "\n",
        "times = sorted(df.index.values)\n",
        "last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n",
        "last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n",
        "\n",
        "df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
        "df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
        "df_test = df[(df.index >= last_10pct)]\n",
        "\n",
        "# Remove date column\n",
        "df_train.drop(columns=['Date'], inplace=True)\n",
        "df_val.drop(columns=['Date'], inplace=True)\n",
        "df_test.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "# Convert pandas columns into arrays\n",
        "train_data = df_train.values\n",
        "val_data = df_val.values\n",
        "test_data = df_test.values\n",
        "print('Training data shape: {}'.format(train_data.shape))\n",
        "print('Validation data shape: {}'.format(val_data.shape))\n",
        "print('Test data shape: {}'.format(test_data.shape))\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moj1_tbiTfdc",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average - Plot daily changes of close price and volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfHJUSEPSaWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,12))\n",
        "st = fig.suptitle(\"Data Separation\", fontsize=20)\n",
        "st.set_y(0.95)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax1.plot(np.arange(train_data.shape[0]), df_train['Close'], label='Training data')\n",
        "\n",
        "ax1.plot(np.arange(train_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]), df_val['Close'], label='Validation data')\n",
        "\n",
        "ax1.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Close'], label='Test data')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Normalized Closing Returns')\n",
        "ax1.set_title(\"Close Price\", fontsize=18)\n",
        "ax1.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "ax2 = fig.add_subplot(212)\n",
        "ax2.plot(np.arange(train_data.shape[0]), df_train['Volume'], label='Training data')\n",
        "\n",
        "ax2.plot(np.arange(train_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]), df_val['Volume'], label='Validation data')\n",
        "\n",
        "ax2.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
        "                   train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Volume'], label='Test data')\n",
        "ax2.set_xlabel('Date')\n",
        "ax2.set_ylabel('Normalized Volume Changes')\n",
        "ax2.set_title(\"Volume\", fontsize=18)\n",
        "ax2.legend(loc=\"best\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkh2TIurTjOO",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average - Create chunks of training, validation, and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4YOIvAuTj4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training data\n",
        "X_train, y_train = [], []\n",
        "for i in range(seq_len, len(train_data)):\n",
        "  X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n",
        "  y_train.append(train_data[:, 3][i]) #Value of 4th column (Close Price) of df-row 128+1\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Validation data\n",
        "X_val, y_val = [], []\n",
        "for i in range(seq_len, len(val_data)):\n",
        "    X_val.append(val_data[i-seq_len:i])\n",
        "    y_val.append(val_data[:, 3][i])\n",
        "X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Test data\n",
        "X_test, y_test = [], []\n",
        "for i in range(seq_len, len(test_data)):\n",
        "    X_test.append(test_data[i-seq_len:i])\n",
        "    y_test.append(test_data[:, 3][i])    \n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns21gno_T2xy",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average - Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1oy3O71Tjyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  '''Initialize time and transformer layers'''\n",
        "  time_embedding = Time2Vector(seq_len)\n",
        "  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "\n",
        "  '''Construct model'''\n",
        "  in_seq = Input(shape=(seq_len, 5))\n",
        "  x = time_embedding(in_seq)\n",
        "  x = Concatenate(axis=-1)([in_seq, x])\n",
        "  x = attn_layer1((x, x, x))\n",
        "  x = attn_layer2((x, x, x))\n",
        "  x = attn_layer3((x, x, x))\n",
        "  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  out = Dense(1, activation='linear')(x)\n",
        "\n",
        "  model = Model(inputs=in_seq, outputs=out)\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
        "  return model\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "callback = tf.keras.callbacks.ModelCheckpoint('Transformer+TimeEmbedding.hdf5', \n",
        "                                              monitor='val_loss', \n",
        "                                              save_best_only=True, \n",
        "                                              verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, \n",
        "                    batch_size=batch_size, \n",
        "                    epochs=35, \n",
        "                    callbacks=[callback],\n",
        "                    validation_data=(X_val, y_val))  \n",
        "\n",
        "model = tf.keras.models.load_model('/content/Transformer+TimeEmbedding.hdf5',\n",
        "                                   custom_objects={'Time2Vector': Time2Vector, \n",
        "                                                   'SingleAttention': SingleAttention,\n",
        "                                                   'MultiAttention': MultiAttention,\n",
        "                                                   'TransformerEncoder': TransformerEncoder})\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "'''Calculate predictions and metrics'''\n",
        "\n",
        "#Calculate predication for training, validation and test data\n",
        "train_pred = model.predict(X_train)\n",
        "val_pred = model.predict(X_val)\n",
        "test_pred = model.predict(X_test)\n",
        "\n",
        "#Print evaluation metrics for all datasets\n",
        "train_eval = model.evaluate(X_train, y_train, verbose=0)\n",
        "val_eval = model.evaluate(X_val, y_val, verbose=0)\n",
        "test_eval = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(' ')\n",
        "print('Evaluation metrics')\n",
        "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
        "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\n",
        "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))\n",
        "\n",
        "###############################################################################\n",
        "'''Display results'''\n",
        "\n",
        "fig = plt.figure(figsize=(15,20))\n",
        "st = fig.suptitle(\"Moving Average - Transformer + TimeEmbedding Model\", fontsize=22)\n",
        "st.set_y(0.92)\n",
        "\n",
        "#Plot training data results\n",
        "ax11 = fig.add_subplot(311)\n",
        "ax11.plot(train_data[:, 3], label='IBM Closing Returns')\n",
        "ax11.plot(np.arange(seq_len, train_pred.shape[0]+seq_len), train_pred, linewidth=3, label='Predicted IBM Closing Returns')\n",
        "ax11.set_title(\"Training Data\", fontsize=18)\n",
        "ax11.set_xlabel('Date')\n",
        "ax11.set_ylabel('IBM Closing Returns')\n",
        "ax11.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot validation data results\n",
        "ax21 = fig.add_subplot(312)\n",
        "ax21.plot(val_data[:, 3], label='IBM Closing Returns')\n",
        "ax21.plot(np.arange(seq_len, val_pred.shape[0]+seq_len), val_pred, linewidth=3, label='Predicted IBM Closing Returns')\n",
        "ax21.set_title(\"Validation Data\", fontsize=18)\n",
        "ax21.set_xlabel('Date')\n",
        "ax21.set_ylabel('IBM Closing Returns')\n",
        "ax21.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot test data results\n",
        "ax31 = fig.add_subplot(313)\n",
        "ax31.plot(test_data[:, 3], label='IBM Closing Returns')\n",
        "ax31.plot(np.arange(seq_len, test_pred.shape[0]+seq_len), test_pred, linewidth=3, label='Predicted IBM Closing Returns')\n",
        "ax31.set_title(\"Test Data\", fontsize=18)\n",
        "ax31.set_xlabel('Date')\n",
        "ax31.set_ylabel('IBM Closing Returns')\n",
        "ax31.legend(loc=\"best\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQxwsG_AT_gm",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average - Model metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCNT45C_TjsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Display model metrics'''\n",
        "\n",
        "fig = plt.figure(figsize=(15,20))\n",
        "st = fig.suptitle(\"Moving Average - Transformer + TimeEmbedding Model Metrics\", fontsize=22)\n",
        "st.set_y(0.92)\n",
        "\n",
        "#Plot model loss\n",
        "ax1 = fig.add_subplot(311)\n",
        "ax1.plot(history.history['loss'], label='Training loss (MSE)')\n",
        "ax1.plot(history.history['val_loss'], label='Validation loss (MSE)')\n",
        "ax1.set_title(\"Model loss\", fontsize=18)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss (MSE)')\n",
        "ax1.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot MAE\n",
        "ax2 = fig.add_subplot(312)\n",
        "ax2.plot(history.history['mae'], label='Training MAE')\n",
        "ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
        "ax2.set_title(\"Model metric - Mean average error (MAE)\", fontsize=18)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Mean average error (MAE)')\n",
        "ax2.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "#Plot MAPE\n",
        "ax3 = fig.add_subplot(313)\n",
        "ax3.plot(history.history['mape'], label='Training MAPE')\n",
        "ax3.plot(history.history['val_mape'], label='Validation MAPE')\n",
        "ax3.set_title(\"Model metric - Mean average percentage error (MAPE)\", fontsize=18)\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Mean average percentage error (MAPE)')\n",
        "ax3.legend(loc=\"best\", fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNvUgwyfTjl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQTPtgI4Tjfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}