{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "torchtext_translation_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wileyw/DeepLearningDemos/blob/master/MachineTranslation/torchtext_translation_tutorial_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w40yexyC0OKu"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXdXe_OiCdWu"
      },
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install torchtext==0.6.0\n",
        "!python -m pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPHfPoKBVhNH"
      },
      "source": [
        "from einops import rearrange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mxEZx8NBCS1"
      },
      "source": [
        "!python -m pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hOubTntBJhG"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download fr\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwmZq0or0OKy"
      },
      "source": [
        "\n",
        "Language Translation with TorchText\n",
        "===================================\n",
        "\n",
        "This tutorial shows how to use several convenience classes of ``torchtext`` to preprocess\n",
        "data from a well-known dataset containing sentences in both English and German and use it to\n",
        "train a sequence-to-sequence model with attention that can translate German sentences\n",
        "into English.\n",
        "\n",
        "- [Link to PyTorch Tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html?highlight=transformer)\n",
        "- [Link to another translation tutorial](https://github.com/andrewpeng02/transformer-translation)\n",
        "\n",
        "It is based off of\n",
        "`this tutorial <https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__\n",
        "from PyTorch community member `Ben Trevett <https://github.com/bentrevett>`__\n",
        "and was created by `Seth Weidman <https://github.com/SethHWeidman/>`__ with Ben's permission.\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "- Preprocess sentences into a commonly-used format for NLP modeling using the following ``torchtext`` convenience classes:\n",
        "    - `TranslationDataset <https://torchtext.readthedocs.io/en/latest/datasets.html#torchtext.datasets.TranslationDataset>`__\n",
        "    - `Field <https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field>`__\n",
        "    - `BucketIterator <https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator>`__\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGgLWgQT0OKz"
      },
      "source": [
        "`Field` and `TranslationDataset`\n",
        "----------------\n",
        "``torchtext`` has utilities for creating datasets that can be easily\n",
        "iterated through for the purposes of creating a language translation\n",
        "model. One key class is a\n",
        "`Field <https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L64>`__,\n",
        "which specifies the way each sentence should be preprocessed, and another is the\n",
        "`TranslationDataset` ; ``torchtext``\n",
        "has several such datasets; in this tutorial we'll use the\n",
        "`Multi30k dataset <https://github.com/multi30k/dataset>`__, which contains about\n",
        "30,000 sentences (averaging about 13 words in length) in both English and German.\n",
        "\n",
        "Note: the tokenization in this tutorial requires `Spacy <https://spacy.io>`__\n",
        "We use Spacy because it provides strong support for tokenization in languages\n",
        "other than English. ``torchtext`` provides a ``basic_english`` tokenizer\n",
        "and supports other tokenizers for English (e.g.\n",
        "`Moses <https://bitbucket.org/luismsgomes/mosestokenizer/src/default/>`__)\n",
        "but for language translation - where multiple languages are required -\n",
        "Spacy is your best bet.\n",
        "\n",
        "To run this tutorial, first install ``spacy`` using ``pip`` or ``conda``.\n",
        "Next, download the raw data for the English and German Spacy tokenizers:\n",
        "\n",
        "::\n",
        "\n",
        "   python -m spacy download en\n",
        "   python -m spacy download de\n",
        "\n",
        "With Spacy installed, the following code will tokenize each of the sentences\n",
        "in the ``TranslationDataset`` based on the tokenizer defined in the ``Field``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrjjUIpfBi_e"
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVjBGHWz0OKz"
      },
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"de\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"en\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHDNc7Tm0OK2"
      },
      "source": [
        "Now that we've defined ``train_data``, we can see an extremely useful\n",
        "feature of ``torchtext``'s ``Field``: the ``build_vocab`` method\n",
        "now allows us to create the vocabulary associated with each language\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0qCZlaB0OK2"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XCxlPKSoc7r"
      },
      "source": [
        "## What does our data look like?\n",
        "Torchtext preprocesses our data to give us a mapping from our source language to the target language and back."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4_tYFC4DnpY"
      },
      "source": [
        "# Printing a list of tokens mapping integer to strings\n",
        "print(SRC.vocab.itos)\n",
        "# Printing a dict mapping tokens to indices\n",
        "print(SRC.vocab.stoi)\n",
        "# Printing the index of an actual word\n",
        "print(SRC.vocab.stoi['ein'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83wQOxw0OK5"
      },
      "source": [
        "Once these lines of code have been run, ``SRC.vocab.stoi`` will  be a\n",
        "dictionary with the tokens in the vocabulary as keys and their\n",
        "corresponding indices as values; ``SRC.vocab.itos`` will be the same\n",
        "dictionary with the keys and values swapped. We won't make extensive\n",
        "use of this fact in this tutorial, but this will likely be useful in\n",
        "other NLP tasks you'll encounter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gLC73JK0OK5"
      },
      "source": [
        "``BucketIterator``\n",
        "----------------\n",
        "The last ``torchtext`` specific feature we'll use is the ``BucketIterator``,\n",
        "which is easy to use since it takes a ``TranslationDataset`` as its\n",
        "first argument. Specifically, as the docs say:\n",
        "Defines an iterator that batches examples of similar lengths together.\n",
        "Minimizes amount of padding needed while producing freshly shuffled\n",
        "batches for each new epoch. See pool for the bucketing procedure used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJXdis5w0OK6"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjhqr8cu0OK8"
      },
      "source": [
        "These iterators can be called just like ``DataLoader``s; below, in\n",
        "the ``train`` and ``evaluate`` functions, they are called simply with:\n",
        "\n",
        "::\n",
        "\n",
        "   for i, batch in enumerate(iterator):\n",
        "\n",
        "Each ``batch`` then has ``src`` and ``trg`` attributes:\n",
        "\n",
        "::\n",
        "\n",
        "   src = batch.src\n",
        "   trg = batch.trg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJwrYvke0OK9"
      },
      "source": [
        "Defining our ``nn.Module`` and ``Optimizer``\n",
        "----------------\n",
        "That's mostly it from a ``torchtext`` perspecive: with the dataset built\n",
        "and the iterator defined, the rest of this tutorial simply defines our\n",
        "model as an ``nn.Module``, along with an ``Optimizer``, and then trains it.\n",
        "\n",
        "Our model specifically, follows the architecture described\n",
        "`here <https://arxiv.org/abs/1409.0473>`__ (you can find a\n",
        "significantly more commented version\n",
        "`here <https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__).\n",
        "\n",
        "Note: this model is just an example model that can be used for language\n",
        "translation; we choose it because it is a standard model for the task,\n",
        "not because it is the recommended model to use for translation. As you're\n",
        "likely aware, state-of-the-art models are currently based on Transformers;\n",
        "you can see PyTorch's capabilities for implementing Transformer layers\n",
        "`here <https://pytorch.org/docs/stable/nn.html#transformer-layers>`__; and\n",
        "in particular, the \"attention\" used in the model below is different from\n",
        "the multi-headed self-attention present in a transformer model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1L2qQrk0OK-"
      },
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "import math\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.src_embedding = nn.Embedding(INPUT_DIM, ENC_EMB_DIM)\n",
        "        self.tgt_embedding = nn.Embedding(INPUT_DIM, ENC_EMB_DIM)\n",
        "        self.transformer = nn.Transformer(nhead=8, num_encoder_layers=2, d_model=ENC_EMB_DIM)\n",
        "        self.linear = nn.Linear(ENC_EMB_DIM, OUTPUT_DIM)\n",
        "        pos_dropout = 0.1\n",
        "        max_seq_length = 128\n",
        "        self.pos_enc = PositionalEncoding(ENC_EMB_DIM, pos_dropout, max_seq_length)\n",
        "    \n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, tgt_mask=None):\n",
        "        # TODO: Investigate masks, positional encoding, understand Rearrange(), debug model output (output has negative numbers for some reason)\n",
        "        # Original src shape: (sentence length=24?, batch_size=128)\n",
        "        # Original tgt shape: (sentence length=24?, batch_size=128)\n",
        "        # Transformer expects: (sentence length=24, batch_size=128, embedding_size=128)\n",
        "        src_emb = self.pos_enc(self.src_embedding(src) * math.sqrt(ENC_EMB_DIM))\n",
        "        tgt_emb = self.pos_enc(self.tgt_embedding(tgt) * math.sqrt(ENC_EMB_DIM))\n",
        "        out = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "model = TransformerModel().to(device)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_normal_(p)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98))\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzRIoUd0OLC"
      },
      "source": [
        "Note: when scoring the performance of a language translation model in\n",
        "particular, we have to tell the ``nn.CrossEntropyLoss`` function to\n",
        "ignore the indices where the target is simply padding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aYGQRq70OLC"
      },
      "source": [
        "# Checking index of special tokens\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "SOS_IDX = TRG.vocab.stoi['<sos>']\n",
        "EOS_IDX = TRG.vocab.stoi['<eos>']\n",
        "UNK_IDX = TRG.vocab.stoi['<unk>']\n",
        "print('pad index:', PAD_IDX)\n",
        "print('sos index:', SOS_IDX)\n",
        "print('eos index:', EOS_IDX)\n",
        "print('unk index:', UNK_IDX)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7FKKikF0OLF"
      },
      "source": [
        "Finally, we can train and evaluate this model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE8O8EKjV7Iw"
      },
      "source": [
        "def gen_nopeek_mask(length):\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uevv7ldzjat4"
      },
      "source": [
        "def example_translate(model, example_sentence_src):\n",
        "    # Translate example sentence\n",
        "    example_tensor_src = string_to_indices(SRC, example_sentence_src).view(-1, 1)\n",
        "    example_sentence_tgt = '<sos>'\n",
        "    example_tensor_tgt = string_to_indices(TRG, example_sentence_tgt).view(-1, 1)\n",
        "    src = example_tensor_src.to(device)\n",
        "    tgt = example_tensor_tgt.to(device)\n",
        "\n",
        "    print('----------Translating--------------')\n",
        "    for i in range(128):\n",
        "        print('Src:', src)\n",
        "        print('Tgt:', tgt)\n",
        "        src_key_padding_mask = src == PAD_IDX\n",
        "        tgt_key_padding_mask = tgt == PAD_IDX\n",
        "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "        src_key_padding_mask = rearrange(src_key_padding_mask, 'n s -> s n')\n",
        "        tgt_key_padding_mask = rearrange(tgt_key_padding_mask, 'n s -> s n')\n",
        "        memory_key_padding_mask = rearrange(memory_key_padding_mask, 'n s -> s n')\n",
        "        tgt_mask = gen_nopeek_mask(tgt.shape[0]).to('cuda')\n",
        "        print('Tgt mask:', tgt_mask)\n",
        "\n",
        "        output = model(src, tgt, 0, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask) #turn off teacher forcing\n",
        "\n",
        "        print('Output:', output)\n",
        "        # TODO: Check that the argmax line is correct\n",
        "        output_index = torch.argmax(output, dim=2)[-1].item()\n",
        "        output_word = TRG.vocab.itos[output_index]\n",
        "        example_sentence_tgt = example_sentence_tgt + ' ' + output_word\n",
        "        print('Translated sentence so far:', example_sentence_tgt)\n",
        "        example_tensor_tgt = string_to_indices(TRG, example_sentence_tgt).view(-1, 1)\n",
        "        tgt = example_tensor_tgt.to(device)\n",
        "        if output_word == '<eos>':\n",
        "            break\n",
        "    print('-----------Finished Translating--------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNfqdy9B0OLG"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def indices_to_string(LANGUAGE, batch):\n",
        "    words_list = []\n",
        "    for sentence in batch.transpose(1, 0):\n",
        "        sentence_list = sentence.tolist()\n",
        "        words = []\n",
        "        for index in sentence_list:\n",
        "            word = LANGUAGE.vocab.itos[index]\n",
        "            words.append(word)\n",
        "        words_list.append(words)\n",
        "    return words_list\n",
        "\n",
        "def string_to_indices(LANGUAGE, sentence):\n",
        "    words = sentence.split()\n",
        "    indices = []\n",
        "    for word in words:\n",
        "        if word in LANGUAGE.vocab.stoi:\n",
        "            index = LANGUAGE.vocab.stoi[word]\n",
        "            indices.append(index)\n",
        "        else:\n",
        "            index = LANGUAGE.vocab.stoi['<unk>']\n",
        "            indices.append(index)\n",
        "    result = torch.tensor(indices)\n",
        "    return result\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        tgt = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Original src shape: (sentence length=24, batch_size=128)\n",
        "        # Transformer expects: (sentence length=24, batch_size=128, embedding_size=128)\n",
        "        src_key_padding_mask = src == PAD_IDX\n",
        "        tgt_key_padding_mask = tgt == PAD_IDX\n",
        "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "        src_key_padding_mask = rearrange(src_key_padding_mask, 'n s -> s n')\n",
        "        tgt_key_padding_mask = rearrange(tgt_key_padding_mask, 'n s -> s n')\n",
        "        memory_key_padding_mask = rearrange(memory_key_padding_mask, 'n s -> s n')\n",
        "        tgt_sentence_len = tgt.shape[0] - torch.sum(tgt_key_padding_mask, axis=1)\n",
        "        tgt_inp, tgt_out = tgt[:-1, :], tgt[1:, :]\n",
        "        tgt_key_padding_mask = tgt_key_padding_mask[:, :-1]\n",
        "        tgt_mask = gen_nopeek_mask(tgt_inp.shape[0]).to('cuda')\n",
        "        output = model(src, tgt_inp, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)\n",
        "        from_one_hot = torch.argmax(output, dim=2)\n",
        "        # output shape: (sentence length=24, batch_size=128, vocab=5893)\n",
        "        # Original tgt shape: (sentence length=24, batch_size=128)\n",
        "\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        tgt_out = tgt_out.view(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_out)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    print('Evaluating')\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            tgt = batch.trg\n",
        "\n",
        "            src_key_padding_mask = src == PAD_IDX\n",
        "            tgt_key_padding_mask = tgt == PAD_IDX\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            src_key_padding_mask = rearrange(src_key_padding_mask, 'n s -> s n')\n",
        "            tgt_key_padding_mask = rearrange(tgt_key_padding_mask, 'n s -> s n')\n",
        "            memory_key_padding_mask = rearrange(memory_key_padding_mask, 'n s -> s n')\n",
        "            tgt_mask = gen_nopeek_mask(tgt.shape[0]).to('cuda')\n",
        "            output = model(src, tgt, 0, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask) #turn off teacher forcing\n",
        "            from_one_hot = torch.argmax(output, dim=2)\n",
        "            #print(src.shape, output.shape, from_one_hot.shape, tgt.shape)\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            #print('Src:', src.transpose(1, 0))\n",
        "            #print('Predicted:', from_one_hot.transpose(1, 0))\n",
        "            #print('Target:', tgt.transpose(1, 0))\n",
        "            src_words = indices_to_string(SRC, src)\n",
        "            predicted_words = indices_to_string(TRG, from_one_hot)\n",
        "            tgt_words = indices_to_string(TRG, tgt)\n",
        "            #for i, src in enumerate(src_words):\n",
        "            #    print('----------------------------------')\n",
        "            #    print(' '.join(src))\n",
        "            #    print(' '.join(predicted_words[i]))\n",
        "            #    print(' '.join(tgt_words[i]))\n",
        "\n",
        "            tgt = tgt[1:].view(-1)\n",
        "            loss = criterion(output, tgt)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BL7yqmWYyMs"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YUFxU6DX6gR"
      },
      "source": [
        "test_loss = evaluate(model, test_iterator, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Mt6ju0mhL2"
      },
      "source": [
        "example_sentence_src = '<sos> der himmel ist blau <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'\n",
        "#example_sentence_src = '<sos> viele menschen haben sich versammelt , um etwas zu sehen , das nicht auf dem foto ist . <eos> <pad> <pad> <pad> <pad>'\n",
        "example_translate(model, example_sentence_src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPsAl8hz0OLI"
      },
      "source": [
        "Next steps\n",
        "--------------\n",
        "\n",
        "- Check out the rest of Ben Trevett's tutorials using ``torchtext``\n",
        "  `here <https://github.com/bentrevett/>`__\n",
        "- Stay tuned for a tutorial using other ``torchtext`` features along\n",
        "  with ``nn.Transformer`` for language modeling via next word prediction!\n",
        "\n",
        "\n"
      ]
    }
  ]
}