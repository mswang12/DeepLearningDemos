{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "simple_audio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wileyw/DeepLearningDemos/blob/master/sound/simple_audio_new_spectrogram_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluF3_oOgkWF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AJs7HHFmg1M9"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Simple audio recognition: Recognizing keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbqmZy0gbyE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfDNFlb66XF"
      },
      "source": [
        "This tutorial will show you how to build a basic speech recognition network that recognizes ten different words. It's important to know that real speech and audio recognition systems are much more complex, but like MNIST for images, it should give you a basic understanding of the techniques involved. Once you've completed this tutorial, you'll have a model that tries to classify a one second audio clip as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" and \"yes\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tqplwVdQ__L"
      },
      "source": [
        "!git clone https://github.com/google-coral/project-keyword-spotter.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F5jI3URRH38"
      },
      "source": [
        "!ls project-keyword-spotter/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AJr9ZolRL0W"
      },
      "source": [
        "!cp project-keyword-spotter/mel_features.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSqyACxRdkQ"
      },
      "source": [
        "!ls\n",
        "import mel_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9C3uLL8Izc"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary modules and dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Import the Speech Commands dataset\n",
        "\n",
        "You'll write a script to download a portion of the [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands). The original dataset consists of over 105,000 WAV audio files of people saying thirty different words. This data was collected by Google and released under a CC BY license, and you can help improve it by [contributing five minutes of your own voice](https://aiyprojects.withgoogle.com/open_speech_recording).\n",
        "\n",
        "You'll be using a portion of the dataset to save time with data loading. Extract the `mini_speech_commands.zip` and load it in using the `tf.data` API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-rayb7-3Y0I"
      },
      "source": [
        "data_dir = pathlib.Path('data/mini_speech_commands')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvFq3uYiS5G"
      },
      "source": [
        "Check basic statistics about the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70IBxSKxA1N9"
      },
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "print('Commands:', commands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMvdU9SY8WXN"
      },
      "source": [
        "Extract the audio files into a list and shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlX685l1wD9k"
      },
      "source": [
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)\n",
        "print('Number of examples per label:',\n",
        "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
        "print('Example file tensor:', filenames[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vK3ymy23MCP"
      },
      "source": [
        "Split the files into training, validation and test sets using a 80:10:10 ratio, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv_wts-l3KgD"
      },
      "source": [
        "train_files = filenames[:6400]\n",
        "val_files = filenames[6400: 6400 + 800]\n",
        "test_files = filenames[-800:]\n",
        "\n",
        "print('Training set size', len(train_files))\n",
        "print('Validation set size', len(val_files))\n",
        "print('Test set size', len(test_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Cj9FyvfweD"
      },
      "source": [
        "## Reading audio files and their labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1zjcWteOcBy"
      },
      "source": [
        "The audio file will initially be read as a binary file, which you'll want to convert into a numerical tensor.\n",
        "\n",
        "To load an audio file, you will use [`tf.audio.decode_wav`](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav), which returns the WAV-encoded audio as a Tensor and the sample rate.\n",
        "\n",
        "A WAV file contains time series data with a set number of samples per second. \n",
        "Each sample represents the amplitude of the audio signal at that specific time. In a 16-bit system, like the files in `mini_speech_commands`, the values range from -32768 to 32767. \n",
        "The sample rate for this dataset is 16kHz.\n",
        "Note that `tf.audio.decode_wav` will normalize the values to the range [-1.0, 1.0]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PjJ2iXYwftD"
      },
      "source": [
        "def decode_audio(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPQseZElOjVN"
      },
      "source": [
        "The label for each WAV file is its parent directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VTtX1nr3YT-"
      },
      "source": [
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "\n",
        "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
        "  # to work in a TensorFlow graph.\n",
        "  return parts[-2] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Y9w_5MOsr-"
      },
      "source": [
        "Let's define a method that will take in the filename of the WAV file and output a tuple containing the audio and labels for supervised training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdgUD5T93NyT"
      },
      "source": [
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  return waveform, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvN8W_dDjYjc"
      },
      "source": [
        "You will now apply `process_path` to build your training set to extract the audio-label pairs and check the results. You'll build the validation and test sets using a similar procedure later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SQl8yXl3kNP"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxGEwvuh2L7"
      },
      "source": [
        "Let's examine a few audio waveforms with their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yuX6Nqzf6wT"
      },
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
        "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  ax.plot(audio.numpy())\n",
        "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  ax.set_title(label)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXPphxm0B4m"
      },
      "source": [
        "## Spectrogram\n",
        "\n",
        "You'll convert the waveform into a spectrogram, which shows frequency changes over time and can be represented as a 2D image. This can be done by applying the short-time Fourier transform (STFT) to convert the audio into the time-frequency domain.\n",
        "\n",
        "A Fourier transform ([`tf.signal.fft`](https://www.tensorflow.org/api_docs/python/tf/signal/fft)) converts a signal to its component frequencies, but loses all time information. The STFT ([`tf.signal.stft`](https://www.tensorflow.org/api_docs/python/tf/signal/stft)) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
        "\n",
        "STFT produces an array of complex numbers representing magnitude and phase. However, you'll only need the magnitude for this tutorial, which can be derived by applying `tf.abs` on the output of `tf.signal.stft`. \n",
        "\n",
        "Choose `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on STFT parameters choice, you can refer to [this video](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe) on audio signal processing. \n",
        "\n",
        "You also want the waveforms to have the same length, so that when you convert it to a spectrogram image, the results will have similar dimensions. This can be done by simply zero padding the audio clips that are shorter than one second.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4CK75DHz_OR"
      },
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Padding for files with less than 16000 samples\n",
        "  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        "\n",
        "  # Concatenate audio with padding so that all audio clips will be of the \n",
        "  # same length\n",
        "  waveform = tf.cast(waveform, tf.float32)\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "      \n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "  return spectrogram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWFcNngKR1al"
      },
      "source": [
        "import numpy as np\n",
        "class Uint8LogMelFeatureExtractor(object):\n",
        "  \"\"\"Provide uint8 log mel spectrogram slices from an AudioRecorder object.\n",
        "  This class provides one public method, get_next_spectrogram(), which gets\n",
        "  a specified number of spectral slices from an AudioRecorder.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_frames_hop=49):\n",
        "    self.spectrogram_window_length_seconds = 0.025\n",
        "    self.spectrogram_hop_length_seconds = 0.010\n",
        "    self.num_mel_bins = 32\n",
        "    self.frame_length_spectra = 98\n",
        "    if self.frame_length_spectra % num_frames_hop:\n",
        "        raise ValueError('Invalid num_frames_hop value (%d), '\n",
        "                         'must devide %d' % (num_frames_hop,\n",
        "                                             self.frame_length_spectra))\n",
        "    self.frame_hop_spectra = num_frames_hop\n",
        "    self._norm_factor = 3\n",
        "    self._clear_buffers()\n",
        "\n",
        "  def _clear_buffers(self):\n",
        "    self._audio_buffer = np.array([], dtype=np.int16).reshape(0, 1)\n",
        "    self._spectrogram = np.zeros((self.frame_length_spectra, self.num_mel_bins),\n",
        "                                 dtype=np.float32)\n",
        "\n",
        "  def _spectrogram_underlap_samples(self, audio_sample_rate_hz):\n",
        "    return int((self.spectrogram_window_length_seconds -\n",
        "                self.spectrogram_hop_length_seconds) * audio_sample_rate_hz)\n",
        "\n",
        "  def _frame_duration_seconds(self, num_spectra):\n",
        "    return (self.spectrogram_window_length_seconds +\n",
        "            (num_spectra - 1) * self.spectrogram_hop_length_seconds)\n",
        "\n",
        "  def _compute_spectrogram(self, audio_samples, audio_sample_rate_hz):\n",
        "    \"\"\"Compute log-mel spectrogram and scale it to uint8.\"\"\"\n",
        "    samples = audio_samples.flatten() / float(2**15)\n",
        "    spectrogram = 30 * (\n",
        "        mel_features.log_mel_spectrogram(\n",
        "            samples,\n",
        "            audio_sample_rate_hz,\n",
        "            log_offset=0.001,\n",
        "            window_length_secs=self.spectrogram_window_length_seconds,\n",
        "            hop_length_secs=self.spectrogram_hop_length_seconds,\n",
        "            num_mel_bins=self.num_mel_bins,\n",
        "            lower_edge_hertz=60,\n",
        "            upper_edge_hertz=3800) - np.log(1e-3))\n",
        "    return spectrogram\n",
        "\n",
        "  def _get_next_spectra(self, recorder, num_spectra):\n",
        "    \"\"\"Returns the next spectrogram.\n",
        "    Compute num_spectra spectrogram samples from an AudioRecorder.\n",
        "    Blocks until num_spectra spectrogram slices are available.\n",
        "    Args:\n",
        "      recorder: an AudioRecorder object from which to get raw audio samples.\n",
        "      num_spectra: the number of spectrogram slices to return.\n",
        "    Returns:\n",
        "      num_spectra spectrogram slices computed from the samples.\n",
        "    \"\"\"\n",
        "    required_audio_duration_seconds = self._frame_duration_seconds(num_spectra)\n",
        "    logger.info(\"required_audio_duration_seconds %f\",\n",
        "                required_audio_duration_seconds)\n",
        "    required_num_samples = int(\n",
        "        np.ceil(required_audio_duration_seconds *\n",
        "                recorder.audio_sample_rate_hz))\n",
        "    logger.info(\"required_num_samples %d, %s\", required_num_samples,\n",
        "                str(self._audio_buffer.shape))\n",
        "    audio_samples = np.concatenate(\n",
        "        (self._audio_buffer,\n",
        "         recorder.get_audio(required_num_samples - len(self._audio_buffer))[0]))\n",
        "    self._audio_buffer = audio_samples[\n",
        "        required_num_samples -\n",
        "        self._spectrogram_underlap_samples(recorder.audio_sample_rate_hz):]\n",
        "    spectrogram = self._compute_spectrogram(\n",
        "        audio_samples[:required_num_samples], recorder.audio_sample_rate_hz)\n",
        "    assert len(spectrogram) == num_spectra\n",
        "    return spectrogram\n",
        "\n",
        "  def get_next_spectrogram(self, recorder):\n",
        "    \"\"\"Get the most recent spectrogram frame.\n",
        "    Blocks until the frame is available.\n",
        "    Args:\n",
        "      recorder: an AudioRecorder instance which provides the audio samples.\n",
        "    Returns:\n",
        "      The next spectrogram frame as a uint8 numpy array.\n",
        "    \"\"\"\n",
        "    assert recorder.is_active\n",
        "    logger.info(\"self._spectrogram shape %s\", str(self._spectrogram.shape))\n",
        "    self._spectrogram[:-self.frame_hop_spectra] = (\n",
        "        self._spectrogram[self.frame_hop_spectra:])\n",
        "    self._spectrogram[-self.frame_hop_spectra:] = (\n",
        "        self._get_next_spectra(recorder, self.frame_hop_spectra))\n",
        "    # Return a copy of the internal state that's safe to persist and won't\n",
        "    # change the next time we call this function.\n",
        "    logger.info(\"self._spectrogram shape %s\", str(self._spectrogram.shape))\n",
        "    spectrogram = self._spectrogram.copy()\n",
        "    spectrogram -= np.mean(spectrogram, axis=0)\n",
        "    if self._norm_factor:\n",
        "      spectrogram /= self._norm_factor * np.std(spectrogram, axis=0)\n",
        "      spectrogram += 1\n",
        "      spectrogram *= 127.5\n",
        "    return np.maximum(0, np.minimum(255, spectrogram)).astype(np.uint8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epNtx_cmRkNP"
      },
      "source": [
        "feature_extractor = Uint8LogMelFeatureExtractor()\n",
        "def get_spectrogram2(waveform):\n",
        "    \"\"\"\n",
        "    # Padding for files with less than 16000 samples\n",
        "  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        "\n",
        "  # Concatenate audio with padding so that all audio clips will be of the \n",
        "  # same length\n",
        "  waveform = tf.cast(waveform, tf.float32)\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "      \n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "  return spectrogram\n",
        "    \"\"\"\n",
        "    waveform = waveform.numpy()\n",
        "    #print(waveform.shape)\n",
        "    #print(type(waveform))\n",
        "    spectrogram = feature_extractor._compute_spectrogram(waveform, 16000)\n",
        "    return spectrogram\n",
        "\n",
        "for waveform, label in waveform_ds.take(1):\n",
        "    label2 = label.numpy().decode('utf-8')\n",
        "    spectrogram2 = get_spectrogram2(waveform)\n",
        "print('Label:', label2)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram2.shape)\n",
        "print('Spectrogram type:', spectrogram2.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdPiPYJphs2"
      },
      "source": [
        "Next, you will explore the data. Compare the waveform, the spectrogram and the actual audio of one example from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu6Y7Yz3C-V"
      },
      "source": [
        "for waveform, label in waveform_ds.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "print('Label:', label)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)\n",
        "print('Audio playback')\n",
        "print('Spectrogram type:', spectrogram.dtype)\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62jzb36-Jog"
      },
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  # Convert to frequencies to log scale and transpose so that the time is\n",
        "  # represented in the x-axis (columns).\n",
        "  log_spec = np.log(spectrogram.T)\n",
        "  height = log_spec.shape[0]\n",
        "  X = np.arange(16000, step=height + 1)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYXjW07jCHA"
      },
      "source": [
        "Now transform the waveform dataset to have spectrogram images and their corresponding labels as integer IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43IS2IouEV40"
      },
      "source": [
        "def get_spectrogram_and_label_id(audio, label):\n",
        "  spectrogram = get_spectrogram(audio)\n",
        "  spectrogram = tf.expand_dims(spectrogram, -1)\n",
        "  label_id = tf.argmax(label == commands)\n",
        "  return spectrogram, label_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEVb_oK0oBLQ"
      },
      "source": [
        "spectrogram_ds = waveform_ds.map(\n",
        "    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQpAAgMnyDi"
      },
      "source": [
        "Examine the spectrogram \"images\" for different samples of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUbHfTuon4iF"
      },
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n",
        "  ax.set_title(commands[label_id.numpy()])\n",
        "  ax.axis('off')\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KdY8IF8rkt"
      },
      "source": [
        "## Build and train the model\n",
        "\n",
        "Now you can build and train your model. But before you do that, you'll need to repeat the training set preprocessing on the validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10UI32QH_45b"
      },
      "source": [
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
        "  output_ds = output_ds.map(\n",
        "      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNv4xwYkB2P6"
      },
      "source": [
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e9yyQZuYzYx"
      },
      "source": [
        "def only_load_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds\n",
        "\n",
        "train_waveform_data = only_load_dataset(train_files)\n",
        "val_waveform_data = only_load_dataset(val_files)\n",
        "test_waveform_data = only_load_dataset(test_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assnWo6SB3lR"
      },
      "source": [
        "Batch the training and validation sets for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgY9WYzn61EX"
      },
      "source": [
        "batch_size = 64\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1uIh6F_TN9"
      },
      "source": [
        "Add dataset [`cache()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) and [`prefetch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) operations to reduce read latency while training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdZ6M-F5_QzY"
      },
      "source": [
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwHkKCQQb5oW"
      },
      "source": [
        "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n",
        "The model also has the following additional preprocessing layers:\n",
        "- A [`Resizing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Resizing) layer to downsample the input to enable the model to train faster.\n",
        "- A [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) layer to normalize each pixel in the image based on its mean and standard deviation.\n",
        "\n",
        "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (i.e. mean and standard deviation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALYz7PFCHblP"
      },
      "source": [
        "#for spectrogram, _ in spectrogram_ds.take(1):\n",
        "#  input_shape = spectrogram.shape\n",
        "for data_item, label in train_waveform_data.take(1):\n",
        "    spectrogram = feature_extractor._compute_spectrogram(data_item.numpy(), 16000)\n",
        "    input_shape = (spectrogram.shape[0], spectrogram.shape[1], 1)\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "norm_layer = preprocessing.Normalization()\n",
        "norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    preprocessing.Resizing(32, 32), \n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFjj7-EmsTD-"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b62-5k6qVaIM"
      },
      "source": [
        "new_train_data = []\n",
        "new_train_labels = []\n",
        "new_val_data = []\n",
        "new_val_labels = []\n",
        "new_test_data = []\n",
        "new_test_labels = []\n",
        "for data_item, label in train_waveform_data:\n",
        "    spectrogram = feature_extractor._compute_spectrogram(data_item.numpy(), 16000)\n",
        "    label = label.numpy().decode('utf-8')\n",
        "    label_id = tf.argmax(label == commands)\n",
        "    # NOTE: Spectrogram shape is not always the same\n",
        "    if spectrogram.shape[0] != 98:\n",
        "        continue\n",
        "    new_train_data.append(spectrogram)\n",
        "    new_train_labels.append(label_id)\n",
        "for data_item, label in val_waveform_data:\n",
        "    spectrogram = feature_extractor._compute_spectrogram(data_item.numpy(), 16000)\n",
        "    label = label.numpy().decode('utf-8')\n",
        "    label_id = tf.argmax(label == commands)\n",
        "    if spectrogram.shape[0] != 98:\n",
        "        continue\n",
        "    new_val_data.append(spectrogram)\n",
        "    new_val_labels.append(label_id)\n",
        "for data_item, label in test_waveform_data:\n",
        "    spectrogram = feature_extractor._compute_spectrogram(data_item.numpy(), 16000)\n",
        "    label = label.numpy().decode('utf-8')\n",
        "    label_id = tf.argmax(label == commands)\n",
        "    if spectrogram.shape[0] != 98:\n",
        "        continue\n",
        "    new_test_data.append(spectrogram)\n",
        "    new_test_labels.append(label_id)\n",
        "\n",
        "new_train_data = np.array(new_train_data).astype('float32')\n",
        "new_val_data = np.array(new_val_data).astype('float32')\n",
        "new_test_data = np.array(new_test_data).astype('float32')\n",
        "\n",
        "\n",
        "new_train_labels = np.array(new_train_labels)\n",
        "new_val_labels = np.array(new_val_labels)\n",
        "new_test_labels = np.array(new_test_labels)\n",
        "\n",
        "new_train_data = np.expand_dims(new_train_data, axis=3)\n",
        "new_val_data = np.expand_dims(new_val_data, axis=3)\n",
        "new_test_data = np.expand_dims(new_test_data, axis=3)\n",
        "print('--------')\n",
        "print(new_train_data.shape)\n",
        "print(new_val_data.shape)\n",
        "print(new_test_data.shape)\n",
        "print(new_train_labels.shape)\n",
        "print(new_val_labels.shape)\n",
        "print(new_test_labels.shape)\n",
        "print('--------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttioPJVMcGtq"
      },
      "source": [
        "EPOCHS = 10\n",
        "#history = model.fit(\n",
        "#    train_ds, \n",
        "#    validation_data=val_ds,  \n",
        "#    epochs=EPOCHS,\n",
        "#    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        "#)\n",
        "history = model.fit(\n",
        "    new_train_data, new_train_labels,\n",
        "    validation_data=(new_val_data, new_val_labels),  \n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpCDeQ4mUfS"
      },
      "source": [
        "Let's check the training and validation loss curves to see how your model has improved during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzhipg3Gu2AY"
      },
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTt3kO3mfm4"
      },
      "source": [
        "## Evaluate test set performance\n",
        "\n",
        "Let's run the model on the test set and check performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biU2MwzyAo8o"
      },
      "source": [
        "#test_audio = []\n",
        "#test_labels = []\n",
        "\n",
        "#for audio, label in test_ds:\n",
        "#  test_audio.append(audio.numpy())\n",
        "#  test_labels.append(label.numpy())\n",
        "\n",
        "#test_audio = np.array(test_audio)\n",
        "#test_labels = np.array(test_labels)\n",
        "test_audio = new_test_data\n",
        "test_labels = new_test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktUanr9mRZky"
      },
      "source": [
        "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9Znt1NOabH"
      },
      "source": [
        "### Display a confusion matrix\n",
        "\n",
        "A confusion matrix is helpful to see how well the model did on each of the commands in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvoSAOiXU3lL"
      },
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, xticklabels=commands, yticklabels=commands, \n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGi_mzPcLvl"
      },
      "source": [
        "## Run inference on an audio file\n",
        "\n",
        "Finally, verify the model's prediction output using an input audio file of someone saying \"no.\" How well does your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvhqyCJIjJyp"
      },
      "source": [
        "!ls data/mini_speech_commands/up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRxauKMdhofU"
      },
      "source": [
        "sample_file = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
        "#sample_file = data_dir/'no/ac7840d8_nohash_1.wav'\n",
        "#sample_file = data_dir/'no/5588c7e6_nohash_0.wav'\n",
        "#sample_file = data_dir/'up/52e228e9_nohash_0.wav'\n",
        "\n",
        "#sample_ds = preprocess_dataset([str(sample_file)])\n",
        "X = only_load_dataset([str(sample_file)])\n",
        "for waveform, label in X.take(1):\n",
        "    label = label.numpy().decode('utf-8')\n",
        "    print(waveform, label)\n",
        "\n",
        "    spectrogram = feature_extractor._compute_spectrogram(waveform.numpy(), 16000)\n",
        "    # NOTE: Dimensions need to be expanded\n",
        "    spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
        "    spectrogram = np.expand_dims(spectrogram, axis=0)\n",
        "    print(spectrogram.shape)\n",
        "    prediction = model(spectrogram)\n",
        "    print(prediction.shape)\n",
        "\n",
        "    plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
        "    plt.title(f'Predictions for \"{label}\"')\n",
        "    plt.show()\n",
        "#for spectrogram, label in sample_ds.batch(1):\n",
        "#  prediction = model(spectrogram)\n",
        "#  plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
        "#  plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
        "#  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a7xRQPvYwnW"
      },
      "source": [
        "print(model)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JaL8Tf9ZS83"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EnXaa-8aAsc"
      },
      "source": [
        "# https://www.tensorflow.org/lite/guide/inference\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details)\n",
        "print(output_details)\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHUa-11SZhUz"
      },
      "source": [
        "sample_file = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
        "\n",
        "#sample_ds = preprocess_dataset([str(sample_file)])\n",
        "#waveform, label = get_waveform_and_label(sample_file)\n",
        "#spectrogram = feature_extractor._compute_spectrogram(waveform, 16000)\n",
        "\n",
        "X = only_load_dataset([str(sample_file)])\n",
        "for waveform, label in X.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram = feature_extractor._compute_spectrogram(waveform.numpy(), 16000)\n",
        "  spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
        "  spectrogram = np.expand_dims(spectrogram, axis=0)\n",
        "  print('Original--------------------')\n",
        "  print(spectrogram.shape)\n",
        "  prediction = model(spectrogram)\n",
        "  print(prediction)\n",
        "\n",
        "  print('TFLITE--------------------')\n",
        "  # NOTE: dtype needs to be np.float32\n",
        "  input_data = np.array(spectrogram, dtype=np.float32)\n",
        "  print(input_data.shape)\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "  interpreter.invoke()\n",
        "  prediction2 = interpreter.get_tensor(output_details[0]['index'])\n",
        "  print(prediction2)\n",
        "\n",
        "  print(np.argmax(np.array(prediction).flatten()))\n",
        "  print(np.argmax(np.array(prediction2).flatten()))\n",
        "\n",
        "  plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
        "  plt.title(f'Predictions for \"{label}\"')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWICqdqQNaQ"
      },
      "source": [
        "You can see that your model very clearly recognized the audio command as \"no.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvwJZDteCnh"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model.tflite') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3jF933m9z1J"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This tutorial showed how you could do simple audio classification using a convolutional neural network with TensorFlow and Python.\n",
        "\n",
        "* To learn how to use transfer learning for audio classification, check out the [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial.\n",
        "\n",
        "* To build your own interactive web app for audio classification, consider taking the [TensorFlow.js - Audio recognition using transfer learning codelab](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0).\n",
        "\n",
        "* TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n"
      ]
    }
  ]
}