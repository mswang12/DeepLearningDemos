{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNA1LjLyWi3m9zEkFM5TnIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wileyw/DeepLearningDemos/blob/master/RL/RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWNePv92XVNK"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnsKx2qAXilT"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQBQI8-tYfst"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions \n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super(Agent, self).__init__()\n",
        "        self.state_shape = state_shape\n",
        "        self.action_shape = action_shape\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.linear1 = nn.Linear(state_shape, 24)\n",
        "        self.linear2 = nn.Linear(24, 12)\n",
        "        self.linear3 = nn.Linear(12, action_shape)\n",
        "        # self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, state):\n",
        "        # print(state, state.shape)\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        action = F.softmax(self.linear3(x))\n",
        "        # action = self.linear3(x)\n",
        "        \n",
        "        return action\n",
        "\n",
        "def one_hot_encode_action(action, n_actions):\n",
        "    encoded = np.zeros(n_actions, np.float32)\n",
        "    encoded[action] = 1\n",
        "    return encoded\n",
        "\n",
        "def get_discounted_rewards(rewards):\n",
        "    \"\"\"\n",
        "    - The discounted rewards sum up all the rewards in the episode\n",
        "    - Later rewards are exponentially less important\n",
        "    \"\"\"\n",
        "    discounted_rewards = np.zeros(len(rewards))\n",
        "    running_add = 0\n",
        "    gamma = 0.99\n",
        "    for t in reversed(range(0, len(rewards))):\n",
        "        running_add = running_add * gamma + rewards[t]\n",
        "        discounted_rewards[t] = running_add\n",
        "    return discounted_rewards\n",
        "\n",
        "# loss = nn.CrossEntropyLoss()\n",
        "# loss = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk-dop1tSklo"
      },
      "source": [
        "# Setup env.\n",
        "SEED = 1 # 1337\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "env.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# setup model\n",
        "n_states = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "model = Agent(n_states, n_actions)\n",
        "\n",
        "# training settings.\n",
        "n_episodes = 300\n",
        "episode_lengths = []\n",
        "render = False\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq_giyKH_pgj"
      },
      "source": [
        "# Score Function Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU696mJD09V5"
      },
      "source": [
        "running_reward = 10\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    observation = env.reset()\n",
        "    ep_reward = 0\n",
        "\n",
        "    done = False\n",
        "\n",
        "    observations = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    gradients = []\n",
        "    probs = []\n",
        "    log_probs = []\n",
        "\n",
        "    t = 0\n",
        "    while not done:\n",
        "        if render:\n",
        "            screen = env.render(mode='rgb_array')  # Skip if want to train w/o image\n",
        "\n",
        "            plt.imshow(screen)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "        \n",
        "        # observation (4,) -> (1, 4)\n",
        "        observation_reshaped = observation.reshape([1, observation.shape[0]])\n",
        "\n",
        "        # 1. Get next action.\n",
        "        obs_tmp = observation_reshaped.astype(np.float32)\n",
        "        torch_obs = torch.from_numpy(obs_tmp)\n",
        "        action_prob_distribution = model.forward(torch_obs)\n",
        "        m = torch.distributions.Categorical(action_prob_distribution)\n",
        "        action_prob_distribution = action_prob_distribution.flatten().detach().numpy()\n",
        "        action = m.sample()\n",
        "        observation, reward, done, info = env.step(action.item())\n",
        "        ep_reward += reward\n",
        "\n",
        "        # 2. Record history.\n",
        "        observations.append(observation_reshaped)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        probs.append(action_prob_distribution)\n",
        "        log_probs.append(m.log_prob(action))\n",
        "        t += 1\n",
        "\n",
        "        if done:\n",
        "            print('Episode finished after {} timesteps'.format(t))\n",
        "            episode_lengths.append(t + 1)\n",
        "            print('Average Episode Length: {} from n_episodes: {}'.format(np.mean(episode_lengths), episode))\n",
        "            # 3. Update policy\n",
        "            discounted_rewards = get_discounted_rewards(rewards)\n",
        "            discounted_rewards -= np.mean(discounted_rewards)\n",
        "            discounted_rewards /= np.std(discounted_rewards + 1e-7)\n",
        "            discounted_rewards = torch.Tensor(discounted_rewards)\n",
        "\n",
        "            loss = [-log_prob * r for log_prob, r in zip(log_probs, discounted_rewards)]\n",
        "            loss = torch.cat(loss).sum()\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            observations = []\n",
        "            actions = []\n",
        "            rewards = []\n",
        "            gradients = []\n",
        "            probs = []\n",
        "            log_probs = []\n",
        "            break\n",
        "\n",
        "    # running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "    # if episode % 10 == 0:\n",
        "    #     print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "    #             episode, ep_reward, running_reward))\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eefwxKWOT35J"
      },
      "source": [
        "model.eval()\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "# env.seed(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# prev_screen = env.render(mode='rgb_array')\n",
        "# plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(1000):\n",
        "    # action = env.action_space.sample()\n",
        "    obs_tmp = obs.reshape([1, observation.shape[0]])\n",
        "    obs_tmp = obs_tmp.astype(np.float32)\n",
        "    torch_obs = torch.from_numpy(obs_tmp)\n",
        "    action_prob_distribution = model.forward(torch_obs)\n",
        "    m = torch.distributions.categorical.Categorical(action_prob_distribution)\n",
        "    action = m.sample()\n",
        "    # print(action_prob_distribution.detach().numpy(), action.item())\n",
        "    obs, reward, done, info = env.step(action.item())\n",
        "\n",
        "    # screen = env.render(mode='rgb_array')  # Skip if want to run w/o image\n",
        "    # plt.imshow(screen)\n",
        "    # ipythondisplay.clear_output(wait=True)\n",
        "    # ipythondisplay.display(plt.gcf())\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(i + 1)\n",
        "# print(obs, reward, action)\n",
        "# print(action, action_prob_distribution, action1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7An5boGg_j3X"
      },
      "source": [
        "# Reparameterization Trick Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5lv1KGCGQ6"
      },
      "source": [
        "class ReparamTrickAgent(nn.Module):\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super(ReparamTrickAgent, self).__init__()\n",
        "        self.state_shape = state_shape\n",
        "        self.action_shape = action_shape\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.linear1 = nn.Linear(state_shape, 24)\n",
        "        self.linear2 = nn.Linear(24, 12)\n",
        "\n",
        "        init_w = 3e-3\n",
        "        self.mu_linear = nn.Linear(12, action_shape)\n",
        "        self.mu_linear.weight.data.uniform_(-init_w, init_w)\n",
        "        self.mu_linear.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "        self.logvar_linear = nn.Linear(12, action_shape)\n",
        "        self.logvar_linear.weight.data.uniform_(-init_w, init_w)\n",
        "        self.logvar_linear.bias.data.uniform_(-init_w, init_w)\n",
        "        # self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, state):\n",
        "        # print(state, state.shape)\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        # action = F.softmax(self.linear3(x))\n",
        "        mu = torch.tanh(self.mu_linear(x))\n",
        "        logvar = torch.tanh(self.logvar_linear(x))\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        \n",
        "        return mu, std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DAQZSwG_iGm"
      },
      "source": [
        "# Setup env.\n",
        "SEED = 1 # 1337\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "env.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# setup model\n",
        "n_states = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "reparam_model = ReparamTrickAgent(n_states, n_actions)\n",
        "\n",
        "# training settings.\n",
        "n_episodes = 1000\n",
        "episode_lengths = []\n",
        "render = False\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(reparam_model.parameters(), lr=learning_rate)\n",
        "\n",
        "running_reward = 10\n",
        "for episode in range(1, n_episodes + 1):\n",
        "    observation = env.reset()\n",
        "    ep_reward = 0\n",
        "\n",
        "    done = False\n",
        "\n",
        "    observations = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    gradients = []\n",
        "    probs = []\n",
        "    log_probs = []\n",
        "\n",
        "    t = 0\n",
        "    while not done:\n",
        "        if render:\n",
        "            screen = env.render(mode='rgb_array')  # Skip if want to train w/o image\n",
        "\n",
        "            plt.imshow(screen)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "        \n",
        "        # observation (4,) -> (1, 4)\n",
        "        observation_reshaped = observation.reshape([1, observation.shape[0]])\n",
        "\n",
        "        # 1. Get next action.\n",
        "        obs_tmp = observation_reshaped.astype(np.float32)\n",
        "        torch_obs = torch.from_numpy(obs_tmp)\n",
        "        mu, std = reparam_model.forward(torch_obs)\n",
        "        m = torch.distributions.Normal(mu, std)\n",
        "        # action = m.rsample()\n",
        "        action = m.sample()\n",
        "        log_prob = m.log_prob(action).sum(axis=-1) \n",
        "        # print(log_prob)\n",
        "        action = torch.argmax(action)\n",
        "        # print(action)\n",
        "        observation, reward, done, info = env.step(action.item())\n",
        "        ep_reward += reward\n",
        "\n",
        "        # 2. Record history.\n",
        "        observations.append(observation_reshaped)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        probs.append(action_prob_distribution)\n",
        "        log_probs.append(log_prob)\n",
        "        t += 1\n",
        "\n",
        "        if done:\n",
        "            print('Episode finished after {} timesteps'.format(t))\n",
        "            episode_lengths.append(t + 1)\n",
        "            print('Average Episode Length: {} from n_episodes: {}'.format(np.mean(episode_lengths), episode))\n",
        "            # 3. Update policy\n",
        "            discounted_rewards = get_discounted_rewards(rewards)\n",
        "            discounted_rewards -= np.mean(discounted_rewards)\n",
        "            discounted_rewards /= np.std(discounted_rewards + 1e-7)\n",
        "            discounted_rewards = torch.Tensor(discounted_rewards)\n",
        "\n",
        "            # actions = torch.Tensor(actions)\n",
        "            loss = [-log_prob * r for log_prob, r in zip(log_probs, discounted_rewards)]\n",
        "            loss = torch.cat(loss).sum()\n",
        "            # print(rewards)\n",
        "            # print(discounted_rewards)\n",
        "            # loss = discounted_rewards.sum()\n",
        "            # print(loss)\n",
        "            # loss = -(log_prob * discounted_rewards).mean()\n",
        "\n",
        "            # Useful extra info\n",
        "            # approx_kl = (logp_old - logp).mean().item()\n",
        "            # ent = pi.entropy().mean().item()\n",
        "            # pi_info = dict(kl=approx_kl, ent=ent)\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            observations = []\n",
        "            actions = []\n",
        "            rewards = []\n",
        "            gradients = []\n",
        "            probs = []\n",
        "            log_probs = []\n",
        "            break\n",
        "\n",
        "    # running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "    # if episode % 10 == 0:\n",
        "    #     print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "    #             episode, ep_reward, running_reward))\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyJEYH2WYXIR"
      },
      "source": [
        "reparam_model.eval()\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "# env.seed(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# prev_screen = env.render(mode='rgb_array')\n",
        "# plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(1000):\n",
        "    # action = env.action_space.sample()\n",
        "    obs_tmp = obs.reshape([1, observation.shape[0]])\n",
        "    obs_tmp = obs_tmp.astype(np.float32)\n",
        "    torch_obs = torch.from_numpy(obs_tmp)\n",
        "    mu, std = reparam_model.forward(torch_obs)\n",
        "    m = torch.distributions.Normal(mu, std)\n",
        "    action = torch.argmax(m.sample())\n",
        "    # print(action_prob_distribution.detach().numpy(), action.item())\n",
        "    obs, reward, done, info = env.step(action.item())\n",
        "\n",
        "    # screen = env.render(mode='rgb_array')  # Skip if want to run w/o image\n",
        "    # plt.imshow(screen)\n",
        "    # ipythondisplay.clear_output(wait=True)\n",
        "    # ipythondisplay.display(plt.gcf())\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(i + 1)\n",
        "# print(obs, reward, action)\n",
        "# print(action, action_prob_distribution, action1)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}